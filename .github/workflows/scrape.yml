name: Scrape every 6 hours

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: scrape-magangpulse
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-22.04

    env:
      BASE_URL: ${{ vars.BASE_URL || 'https://maganghub.kemnaker.go.id/lowongan' }}
      DB_PATH: backend/data.sqlite
      REQUEST_TIMEOUT: 30
      USE_PLAYWRIGHT: 1         # set ke 0 kalau mau full non-Playwright
      USE_PLAYWRIGHT_DETAIL: 1
      DETAIL_ENRICH: 1
      DETAIL_MAX: 999999
      DETAIL_WORKERS: 6
      MAX_PAGES: 999999
      THROTTLE_SECONDS: 1.0
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      USER_AGENT: ${{ secrets.USER_AGENT }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: |
            requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      # âœ… Install browser Playwright hanya jika dipakai
      - name: Ensure Playwright (deps + Chromium)
        if: env.USE_PLAYWRIGHT == '1'
        run: |
          sudo -E python -m playwright install-deps chromium
          python -m playwright install chromium

      - name: Generate runtime .env
        shell: bash
        run: |
          set -euo pipefail
          {
            echo "BASE_URL=${BASE_URL}"
            echo "DATABASE_URL=${DATABASE_URL}"
            echo "DB_PATH=${DB_PATH}"
            echo "USER_AGENT=${USER_AGENT}"
            echo "REQUEST_TIMEOUT=${REQUEST_TIMEOUT}"
            echo "USE_PLAYWRIGHT=${USE_PLAYWRIGHT}"
            echo "USE_PLAYWRIGHT_DETAIL=${USE_PLAYWRIGHT_DETAIL}"
            echo "DETAIL_ENRICH=${DETAIL_ENRICH}"
            echo "DETAIL_MAX=${DETAIL_MAX}"
            echo "DETAIL_WORKERS=${DETAIL_WORKERS}"
            echo "MAX_PAGES=${MAX_PAGES}"
            echo "THROTTLE_SECONDS=${THROTTLE_SECONDS}"
          } > .env

      - name: Run scraper
        run: |
          python backend/scraper/run_full_scrape.py
        env:
          BASE_URL: ${{ env.BASE_URL }}
          DATABASE_URL: ${{ env.DATABASE_URL }}
          DB_PATH: ${{ env.DB_PATH }}
          USER_AGENT: ${{ env.USER_AGENT }}
          REQUEST_TIMEOUT: ${{ env.REQUEST_TIMEOUT }}
          USE_PLAYWRIGHT: ${{ env.USE_PLAYWRIGHT }}
          USE_PLAYWRIGHT_DETAIL: ${{ env.USE_PLAYWRIGHT_DETAIL }}
          DETAIL_ENRICH: ${{ env.DETAIL_ENRICH }}
          DETAIL_MAX: ${{ env.DETAIL_MAX }}
          DETAIL_WORKERS: ${{ env.DETAIL_WORKERS }}
          MAX_PAGES: ${{ env.MAX_PAGES }}
          THROTTLE_SECONDS: ${{ env.THROTTLE_SECONDS }}

      - name: Upload artifacts (SQLite/CSV/JSON)
        uses: actions/upload-artifact@v4
        with:
          name: scrape-output
          path: |
            backend/*.sqlite*
            backend/data.sqlite
            **/*.csv
            **/*.json
            !node_modules/**
            !**/tmp/**
          if-no-files-found: ignore
          retention-days: 14
