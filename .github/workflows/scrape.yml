name: Scrape every 6 hours

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: scrape-magangpulse
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-22.04
    container: mcr.microsoft.com/playwright/python:v1.47.0-jammy

    env:
      BASE_URL: ${{ vars.BASE_URL || 'https://maganghub.kemnaker.go.id/lowongan' }}
      DB_PATH: backend/data.sqlite
      REQUEST_TIMEOUT: "30"
      USE_PLAYWRIGHT: "1"
      USE_PLAYWRIGHT_DETAIL: "1"
      DETAIL_ENRICH: "1"
      DETAIL_MAX: "999999"
      DETAIL_WORKERS: "6"
      MAX_PAGES: "999999"
      THROTTLE_SECONDS: "1.0"
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      USER_AGENT: ${{ secrets.USER_AGENT }}
      # >>> Kunci supaya `from backend...` ketemu
      PYTHONPATH: ${{ github.workspace }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install "playwright==1.47.0"
          playwright install chromium

      # (Opsional) Pastikan package init ada
      - name: Ensure Python packages exist
        run: |
          mkdir -p backend/scraper
          [ -f backend/__init__.py ] || touch backend/__init__.py
          [ -f backend/scraper/__init__.py ] || touch backend/scraper/__init__.py

      - name: Generate runtime .env
        shell: bash
        run: |
          set -euo pipefail
          {
            echo "BASE_URL=${BASE_URL}"
            echo "DATABASE_URL=${DATABASE_URL}"
            echo "DB_PATH=${DB_PATH}"
            echo "USER_AGENT=${USER_AGENT}"
            echo "REQUEST_TIMEOUT=${REQUEST_TIMEOUT}"
            echo "USE_PLAYWRIGHT=${USE_PLAYWRIGHT}"
            echo "USE_PLAYWRIGHT_DETAIL=${USE_PLAYWRIGHT_DETAIL}"
            echo "DETAIL_ENRICH=${DETAIL_ENRICH}"
            echo "DETAIL_MAX=${DETAIL_MAX}"
            echo "DETAIL_WORKERS=${DETAIL_WORKERS}"
            echo "MAX_PAGES=${MAX_PAGES}"
            echo "THROTTLE_SECONDS=${THROTTLE_SECONDS}"
          } > .env

      - name: Run scraper (module mode)
        run: |
          python -m backend.scraper.run_full_scrape
        env:
          BASE_URL: ${{ env.BASE_URL }}
          DATABASE_URL: ${{ env.DATABASE_URL }}
          DB_PATH: ${{ env.DB_PATH }}
          USER_AGENT: ${{ env.USER_AGENT }}
          REQUEST_TIMEOUT: ${{ env.REQUEST_TIMEOUT }}
          USE_PLAYWRIGHT: ${{ env.USE_PLAYWRIGHT }}
          USE_PLAYWRIGHT_DETAIL: ${{ env.USE_PLAYWRIGHT_DETAIL }}
          DETAIL_ENRICH: ${{ env.DETAIL_ENRICH }}
          DETAIL_MAX: ${{ env.DETAIL_MAX }}
          DETAIL_WORKERS: ${{ env.DETAIL_WORKERS }}
          MAX_PAGES: ${{ env.MAX_PAGES }}
          THROTTLE_SECONDS: ${{ env.THROTTLE_SECONDS }}
          PYTHONPATH: ${{ env.PYTHONPATH }}

      - name: Upload artifacts (SQLite/CSV/JSON)
        uses: actions/upload-artifact@v4
        with:
          name: scrape-output
          path: |
            backend/*.sqlite*
            backend/data.sqlite
            **/*.csv
            **/*.json
            !node_modules/**
            !**/tmp/**
          if-no-files-found: ignore
          retention-days: 14
